import pandas as pd
import requests
import numpy as np
import faiss
import time
from sklearn.cluster import KMeans
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import FAISS
from langchain.llms.base import LLM
from langchain.chains import RetrievalQA

# ========== CONFIG ========== #
GOOGLE_API_KEY = "YOUR_GOOGLE_API_KEY"
GROQ_API_KEY = "YOUR_GROQ_API_KEY"
CSV_PATH = "data.csv"
LLAMA_MODEL = "llama3-8b-8192"

# ========== Helper Function - Retry API Calls ========== #
def call_with_retries(func, retries=3, wait=5):
    for attempt in range(retries):
        try:
            return func()
        except Exception as e:
            print(f"[Retry {attempt+1}/{retries}] Failed: {e}")
            if attempt < retries - 1:
                time.sleep(wait)
    raise RuntimeError("API failed after retries")

# ========== Google Embeddings Wrapper ========== #
class GoogleEmbeddings(Embeddings):
    def __init__(self, api_key):
        self.api_key = api_key
        self.url = f"https://generative.googleapis.com/v1/models/text-embeddings-004:predict?key={self.api_key}"

    def get_embeddings(self, texts):
        payload = {"instances": [{"content": t} for t in texts]}
        response = call_with_retries(lambda: requests.post(self.url, json=payload))
        if response.status_code != 200:
            raise RuntimeError(f"Google Embedding API failed: {response.text}")
        return [x['embeddings'] for x in response.json()['predictions']]

    def embed_query(self, text):
        return self.get_embeddings([text])[0]

    def embed_documents(self, texts):
        batch_size = 100
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            all_embeddings.extend(self.get_embeddings(batch))
        return all_embeddings

# ========== Groq Llama Wrapper ========== #
class GroqLlama(LLM):
    def __init__(self, api_key, model=LLAMA_MODEL):
        self.api_key = api_key
        self.model = model

    @property
    def _llm_type(self):
        return "groq-llama"

    def _call(self, prompt, stop=None):
        url = "https://api.groq.com/openai/v1/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant working with structured data."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.0
        }
        response = call_with_retries(lambda: requests.post(url, json=payload, headers=headers))
        if response.status_code != 200:
            raise RuntimeError(f"Groq API failed: {response.text}")
        return response.json()['choices'][0]['message']['content']

# ========== Load and Prepare Data ========== #
df = pd.read_csv(CSV_PATH)

# Each row becomes a single "document" by flattening all columns into text
docs = []
metadata = []

for idx, row in df.iterrows():
    doc_text = " | ".join([f"{col}: {row[col]}" for col in df.columns])
    docs.append(doc_text)
    metadata.append(row.to_dict())

# ========== Compute Embeddings ========== #
embeddings = GoogleEmbeddings(api_key=GOOGLE_API_KEY).embed_documents(docs)

# ========== Cluster Rows into Groups ========== #
num_clusters = max(10, len(docs) // 100)
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(np.array(embeddings).astype('float32'))

# ========== Aggregate Rows per Cluster into Cluster Documents ========== #
cluster_docs = []
cluster_meta = []

for cluster_id in range(num_clusters):
    cluster_indices = np.where(clusters == cluster_id)[0]
    cluster_text = "\n".join([docs[i] for i in cluster_indices])
    cluster_docs.append(cluster_text)
    cluster_meta.append({
        "cluster": cluster_id,
        "rows_in_cluster": len(cluster_indices),
        "row_range": f"{cluster_indices.min()}-{cluster_indices.max()}"
    })

# ========== Create Vector Store for Clusters ========== #
vectorstore = FAISS.from_texts(cluster_docs, GoogleEmbeddings(api_key=GOOGLE_API_KEY), metadatas=cluster_meta)

# ========== Set Up Retrieval-Augmented QA ========== #
qa_chain = RetrievalQA(llm=GroqLlama(api_key=GROQ_API_KEY), retriever=vectorstore.as_retriever())

# ========== Run Query ========== #
query = "List all members admitted after January 2024."
retrieved = vectorstore.similarity_search(query, k=3)

# Create a better prompt with real context
retrieved_text = "\n\n".join([doc.page_content for doc in retrieved])
final_prompt = (
    f"The following documents were retrieved from a structured data table:\n\n"
    f"{retrieved_text}\n\n"
    f"Now answer the question based on these documents: {query}"
)

# Ask Groq's Llama and print the answer
answer = qa_chain.run(final_prompt)
print("\n===== Final Answer =====\n")
print(answer)
